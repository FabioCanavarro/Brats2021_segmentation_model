{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2542390,"sourceType":"datasetVersion","datasetId":1541666}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-03T14:59:04.652980Z","iopub.execute_input":"2024-07-03T14:59:04.653368Z","iopub.status.idle":"2024-07-03T14:59:04.676436Z","shell.execute_reply.started":"2024-07-03T14:59:04.653339Z","shell.execute_reply":"2024-07-03T14:59:04.675140Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/BraTS2021_00495.tar\n/kaggle/input/BraTS2021_Training_Data.tar\n/kaggle/input/BraTS2021_00621.tar\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n# detect and init the TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    # instantiate a distribution strategy\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.TPUStrategy(tpu)\nexcept:\n    pass\n%pip install glob\n%pip install nibabel","metadata":{"execution":{"iopub.status.busy":"2024-07-03T14:59:04.805740Z","iopub.execute_input":"2024-07-03T14:59:04.807036Z","iopub.status.idle":"2024-07-03T14:59:23.808997Z","shell.execute_reply.started":"2024-07-03T14:59:04.806997Z","shell.execute_reply":"2024-07-03T14:59:23.807305Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for glob\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: nibabel in /opt/conda/lib/python3.10/site-packages (5.2.1)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from nibabel) (1.26.4)\nRequirement already satisfied: packaging>=17 in /opt/conda/lib/python3.10/site-packages (from nibabel) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17->nibabel) (3.1.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport tarfile\nfile = tarfile.open('/kaggle/input/BraTS2021_Training_Data.tar')\n\nfile.extractall('./BraTS2021_Training_Data')\nfile.close()\nfrom tensorflow.keras import mixed_precision\n\npolicy = mixed_precision.Policy('mixed_float32')\nmixed_precision.set_global_policy(policy)\ntf.keras.backend.set_floatx('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages are logged (default behavior)\n                                          # 1 = INFO messages are not printed\n                                          # 2 = INFO and WARNING messages are not printed\n                                          # 3 = INFO, WARNING, and ERROR messages are not printed\nimport tensorflow as tf\ntf.config.run_functions_eagerly(False)\ntf.get_logger().setLevel('ERROR')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T15:01:38.093224Z","iopub.execute_input":"2024-07-03T15:01:38.093768Z","iopub.status.idle":"2024-07-03T15:01:38.104395Z","shell.execute_reply.started":"2024-07-03T15:01:38.093700Z","shell.execute_reply":"2024-07-03T15:01:38.103131Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as kr\nimport tensorflow.keras as keras\nimport nibabel as nib\nimport numpy as np\nimport os\nfrom glob import glob\nimport matplotlib.pylab as plt\nimport keras\nimport random\nimport time\nimport math\n\n\n\nfolder_path = [os.path.join(r\"BraTS2021_Training_Data\", i) for i in os.listdir(r\"BraTS2021_Training_Data\")]\nimgs = []\n\n\nfor i in folder_path:\n    imgs.append(glob(i+\"/*\"))\nprint(\"Done\")\nfolder_path = folder_path[1:]\nimgs = imgs[1:]\nprint(imgs[0])\n\n\n\n#input directory and output directory\noutputpath = []\ninputpath = []\nfor j in imgs:\n    for i in j:\n        if \"seg.nii.gz\" in i.split(\"_\"):\n            outputpath.append(i)\nfor j in imgs:\n    temppath = []\n    for i in j:\n        if \"seg.nii.gz\" not in i.split(\"_\"):\n            temppath.append(i)\n    inputpath.append(temppath)\n    \nx_train_path = inputpath[:1126].copy()\ny_train_path = outputpath[:1126].copy()\nx_eval_path = inputpath[1126:1189].copy()\ny_eval_path = outputpath[1126:1189].copy()\nx_test_path = inputpath[1189:].copy()\ny_test_path = outputpath[1189:].copy()  \n\nparition = {\"train\": x_train_path, \"validation\": x_eval_path}\nlabel = {\"train\": y_train_path, \"validation\": y_eval_path}\n\n\n\n#set up var\nx_VolSlice_start = 10\nx_VolSlice_end = 150\n\ny_VolSlice_start = 10\ny_VolSlice_end = 150\n\n\n#-----------------------------------------------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------model making--------------------------------------------------------------------------------\n\n\n\nimport tensorflow as tf\n\nclass GradientAccumulationModel(tf.keras.Model):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.n_gradients = tf.Variable(0, dtype=tf.int32, trainable=False)\n        self.n_acum_steps = tf.Variable(4, dtype=tf.int32, trainable=False)\n        self.gradient_accumulation = [tf.Variable(tf.zeros_like(v, dtype=tf.float32), trainable=False) for v in self.trainable_variables]\n\n    def train_step(self, data):\n        self.n_gradients.assign_add(1)\n        x, y = data\n        \n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        \n        gradients = tape.gradient(loss, self.trainable_variables)\n        \n        for i in range(len(self.gradient_accumulation)):\n            self.gradient_accumulation[i].assign_add(gradients[i])\n        \n        tf.cond(tf.equal(self.n_gradients, self.n_acum_steps), \n                lambda: self.apply_accu_gradients(),\n                lambda: tf.constant(0))\n        \n        self.compiled_metrics.update_state(y, y_pred)\n        return {m.name: m.result() for m in self.metrics}\n\n    def apply_accu_gradients(self):\n        self.optimizer.apply_gradients(zip(self.gradient_accumulation, self.trainable_variables))\n        \n        for i in range(len(self.gradient_accumulation)):\n            self.gradient_accumulation[i].assign(tf.zeros_like(self.trainable_variables[i], dtype=tf.float32))\n        \n        self.n_gradients.assign(0)\n        return tf.constant(1)\n\n#note:\n# conv3d need a 5d tensor (height,width,depth,channels)\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\n# dice loss as defined above for 4 classes\ndef dice_coefficient(y_true, y_pred, smooth=1):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    y_true_f = tf.keras.backend.flatten(y_true)\n    y_pred_f = tf.keras.backend.flatten(y_pred)\n    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n\n\n \n# These functions are used for evaluating the performance of a segmentation model on three different classes\n# in medical imaging (presumably related to brain tumor segmentation).\n# Input Parameters:\n# y_true: The ground truth segmentation mask for the edema class.\n# y_pred: The predicted segmentation mask for the edema class.\n# epsilon: A small constant to avoid division by zero.\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nK = keras.backend\n\ndef dice_coef(y_true, y_pred, smooth=1.):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n# Update other metric functions similarly\ndef dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n\ndef dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n\ndef dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n\n# Update other metric functions if necessary\ndef precision(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef sensitivity(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives / (possible_positives + K.epsilon())\n\ndef specificity(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives / (possible_negatives + K.epsilon())\n\n\nfrom tensorflow.keras import layers\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\nclass PadCropLayer(layers.Layer):\n    def __init__(self, target_shape, **kwargs):\n        super(PadCropLayer, self).__init__(**kwargs)\n        self.target_shape = target_shape\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        paddings = []\n        for i in range(1, 4):  # For H, W, D dimensions\n            diff = self.target_shape[i] - input_shape[i]\n            pad_before = diff // 2\n            pad_after = diff - pad_before\n            paddings.append([tf.maximum(0, pad_before), tf.maximum(0, pad_after)])\n        paddings = [[0, 0]] + paddings + [[0, 0]]  # Add padding for batch and channel dimensions\n        \n        padded = tf.pad(inputs, paddings)\n        \n        # Crop if necessary\n        cropped = padded[:, :self.target_shape[1], :self.target_shape[2], :self.target_shape[3], :]\n        \n        return cropped\n\ndef conv_block_3d(input_tensor, num_filters, kernel_size=3, dropout_rate=0.3):\n    x = layers.Conv3D(num_filters, kernel_size, padding='same')(input_tensor)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Conv3D(num_filters, kernel_size, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    return x\n\ndef unet_3d(input_shape=(156, 156, 140, 4), num_classes=4):\n    inputs = layers.Input(input_shape)\n    \n    # Encoder (downsampling)\n    conv1 = conv_block_3d(inputs, 32)\n    pool1 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n    \n    conv2 = conv_block_3d(pool1, 64)\n    pool2 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n    \n    conv3 = conv_block_3d(pool2, 128)\n    pool3 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n    \n    conv4 = conv_block_3d(pool3, 256)\n    pool4 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv4)\n    \n    # Bridge\n    conv5 = conv_block_3d(pool4, 512)\n    \n    # Decoder (upsampling)\n    up6 = layers.Conv3DTranspose(256, 2, strides=(2, 2, 2), padding='same')(conv5)\n    up6 = PadCropLayer(tf.shape(conv4))(up6)\n    concat6 = layers.Concatenate()([up6, conv4])\n    conv6 = conv_block_3d(concat6, 256)\n    \n    up7 = layers.Conv3DTranspose(128, 2, strides=(2, 2, 2), padding='same')(conv6)\n    up7 = PadCropLayer(tf.shape(conv3))(up7)\n    concat7 = layers.Concatenate()([up7, conv3])\n    conv7 = conv_block_3d(concat7, 128)\n    \n    up8 = layers.Conv3DTranspose(64, 2, strides=(2, 2, 2), padding='same')(conv7)\n    up8 = PadCropLayer(tf.shape(conv2))(up8)\n    concat8 = layers.Concatenate()([up8, conv2])\n    conv8 = conv_block_3d(concat8, 64)\n    \n    up9 = layers.Conv3DTranspose(32, 2, strides=(2, 2, 2), padding='same')(conv8)\n    up9 = PadCropLayer(tf.shape(conv1))(up9)\n    concat9 = layers.Concatenate()([up9, conv1])\n    conv9 = conv_block_3d(concat9, 32)\n    \n    outputs = layers.Conv3D(num_classes, 1, activation='softmax')(conv9)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Create the model\nmodel = unet_3d(input_shape=(156, 156, 140, 4), num_classes=4)\n\n# Define loss function\ndef weighted_categorical_crossentropy(class_weights):\n    def loss(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        weighted_losses = y_true * tf.expand_dims(class_weights, 0) * tf.math.log(y_pred + 1e-7)\n        return -tf.reduce_sum(weighted_losses, axis=-1)\n    return loss\n\ndef dice_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=[1,2,3,4])\n    denominator = tf.reduce_sum(y_true + y_pred, axis=[1,2,3,4])\n    return 1 - numerator / (denominator + tf.keras.backend.epsilon())\n\ndef combined_loss(y_true, y_pred):\n    class_weights = tf.constant([0.1, 1.0, 1.0, 1.0])\n    wce = weighted_categorical_crossentropy(class_weights)(y_true, y_pred)\n    dice = dice_loss(y_true, y_pred)\n    return wce + dice\n\n\n\n# Define callbacks\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 0.001 * (0.1 ** (epoch // 30))\n)\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    \"3D-UNet-weights-{epoch:02d}-{val_accuracy:.3f}.keras\",\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max'\n)\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    restore_best_weights=True\n)\ncsv_logger = tf.keras.callbacks.CSVLogger('training_3D_UNet.log')\n\n# Train the model\n\n\n# Example class weights (adjust based on your data)\nclass_weights = tf.constant([0.1, 1.0, 1.0, 1.0])\n\ntry:\n    with tpu_strategy.scope():\n        model = unet_3d(input_shape=(156, 156, 140, 4), num_classes=4)\n        model.compile(\n    loss=combined_loss,\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=4),\n             dice_coefficient, precision, sensitivity, specificity,\n             dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing]\n)\n    tf.debugging.set_log_device_placement(True)\nexcept:\n    model = unet_3d(input_shape=(156, 156, 140, 4), num_classes=4)\n    model.compile(\n    loss=combined_loss,\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=4),\n             dice_coefficient, precision, sensitivity, specificity,\n             dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing]\n)\n    tf.debugging.set_log_device_placement(True)\n    \nmodel.summary()\n\n\n\n\n\n# In your __data_generation method, after loading and preprocessing:\n\n\n#datagen\nclass DataGenerator(keras.utils.Sequence):\n    def __init__(self, list_IDs, labels, batch_size=5, dim=(156,156,140), n_channels=4,\n                 n_classes=4, shuffle=True, real_batchsize_custom=2, frames_chunk=18,\n                 y_VolSlice_start=0, y_VolSlice_end=None, **kwargs):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.real_batchsize_custom = real_batchsize_custom\n        self.frames_chunk = frames_chunk\n        self.y_VolSlice_start = y_VolSlice_start\n        self.y_VolSlice_end = y_VolSlice_end if y_VolSlice_end is not None else dim[2]\n        super().__init__(**kwargs)\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = np.empty((self.real_batchsize_custom, *self.dim, self.n_channels))\n        y = np.empty((self.real_batchsize_custom, *self.dim, self.n_classes), dtype=float)\n\n        initial_point = 0\n        final_point = self.frames_chunk\n        for x in range(self.real_batchsize_custom):    \n            indexes_orig = indexes[initial_point:final_point]\n            list_IDs_temp = [self.list_IDs[k] for k in indexes_orig]\n            X_in, y_in = self.__data_generation(list_IDs_temp)\n            X[x] = X_in\n            y[x] = y_in\n            initial_point = final_point\n            final_point = final_point + self.frames_chunk   \n        return X, y\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples'\n        X = np.empty((1, *self.dim, self.n_channels))\n        y = np.zeros((1, *self.dim, self.n_classes))\n        \n        for i, file_paths in enumerate(list_IDs_temp):\n            image = self.load_nifti(file_paths)\n            X[0] = image\n            \n            seg = nib.load(self.labels[i]).get_fdata().astype(np.float16)\n            seg = self.preprocess_mask(seg)\n            \n            y[0] = seg\n        \n        X = X.astype(np.float16)\n        y = y.astype(np.float16)\n        X[0], y[0] = self.augment(X[0], y[0])\n        X_norm = self.normalize(X[0])\n        return X_norm, y[0]\n\n    def load_nifti(self, file_paths):\n        image = np.empty((*self.dim, self.n_channels))\n        for channel, file_path in enumerate(file_paths):\n            img = nib.load(file_path).get_fdata().astype(np.float16)\n            img = self.preprocess_image(img)\n            image[..., channel] = img\n        return image\n\n    def preprocess_image(self, img):\n        # Resize to match model input dimensions\n        return resize(img, self.dim, order=1, preserve_range=True)\n\n    def preprocess_mask(self, mask):\n        mask[mask==4] = 3\n        mask = resize(mask, self.dim, order=0, preserve_range=True)\n        return tf.one_hot(mask.astype(int), self.n_classes)\n\n    def normalize(self, image):\n        for c in range(image.shape[-1]):\n            channel = image[..., c]\n            min_val = np.min(channel)\n            max_val = np.max(channel)\n            if max_val > min_val:\n                image[..., c] = (channel - min_val) / (max_val - min_val)\n        return image\n\n    def augment(self, image, mask):\n        # Implement data augmentation here\n        return image, mask\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n            \n            \n            \ntraining_generator = DataGenerator(parition[\"train\"], label[\"train\"], dim=(156, 156, 140), n_channels=4, n_classes=4)\nvalid_generator = DataGenerator(parition[\"validation\"], label[\"validation\"], dim=(156, 156, 140), n_channels=4, n_classes=4)\n\n\n#model fitting\n\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n\nfilepath=\"3D-UNet-2018-weights-improvement-{epoch:02d}-{val_accuracy:.3f}.keras\" \n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n\ncsv_logger = CSVLogger('training_2021_2D_UNet.log')\n\nhistory = model.fit(\n    training_generator,\n    epochs=100,\n    steps_per_epoch=len(parition[\"train\"]),\n    validation_data=valid_generator,\n    validation_steps=len(parition[\"validation\"]),\n    callbacks=[checkpoint, early_stop, csv_logger, lr_schedule])\n\nwith open('history.txt', 'w') as f:\n    f.write(history)\ntry:\n    model.save(\"final_model.h5\")\nexcept:\n    model.save(\"final_model.keras\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T16:40:07.254544Z","iopub.execute_input":"2024-07-03T16:40:07.255010Z","iopub.status.idle":"2024-07-03T16:40:08.111575Z","shell.execute_reply.started":"2024-07-03T16:40:07.254971Z","shell.execute_reply":"2024-07-03T16:40:08.109553Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Done\n['BraTS2021_Training_Data/BraTS2021_00622/BraTS2021_00622_t1.nii.gz', 'BraTS2021_Training_Data/BraTS2021_00622/BraTS2021_00622_t1ce.nii.gz', 'BraTS2021_Training_Data/BraTS2021_00622/BraTS2021_00622_t2.nii.gz', 'BraTS2021_Training_Data/BraTS2021_00622/BraTS2021_00622_flair.nii.gz', 'BraTS2021_Training_Data/BraTS2021_00622/BraTS2021_00622_seg.nii.gz']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 269\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Create the model\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43munet_3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m156\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m156\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m140\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Define loss function\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweighted_categorical_crossentropy\u001b[39m(class_weights):\n","Cell \u001b[0;32mIn[26], line 244\u001b[0m, in \u001b[0;36munet_3d\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Decoder (upsampling)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m up6 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv3DTranspose(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m2\u001b[39m, strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)(conv5)\n\u001b[0;32m--> 244\u001b[0m up6 \u001b[38;5;241m=\u001b[39m PadCropLayer(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv4\u001b[49m\u001b[43m)\u001b[49m)(up6)\n\u001b[1;32m    245\u001b[0m concat6 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConcatenate()([up6, conv4])\n\u001b[1;32m    246\u001b[0m conv6 \u001b[38;5;241m=\u001b[39m conv_block_3d(concat6, \u001b[38;5;241m256\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"],"ename":"ValueError","evalue":"A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model","metadata":{}},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}