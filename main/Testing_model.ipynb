{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "['/mnt/d/Fabio/deeplearningdata/archive_1/BraTS2021_Training_Data/BraTS2021_00000/BraTS2021_00000_flair.nii.gz', '/mnt/d/Fabio/deeplearningdata/archive_1/BraTS2021_Training_Data/BraTS2021_00000/BraTS2021_00000_seg.nii.gz', '/mnt/d/Fabio/deeplearningdata/archive_1/BraTS2021_Training_Data/BraTS2021_00000/BraTS2021_00000_t1.nii.gz', '/mnt/d/Fabio/deeplearningdata/archive_1/BraTS2021_Training_Data/BraTS2021_00000/BraTS2021_00000_t1ce.nii.gz', '/mnt/d/Fabio/deeplearningdata/archive_1/BraTS2021_Training_Data/BraTS2021_00000/BraTS2021_00000_t2.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow.keras as kr\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "folder_path = [os.path.join(r\"/mnt/d/Fabio/deeplearningdata/archive_1/BraTS2021_Training_Data\", i) for i in os.listdir(r\"/mnt/d/Fabio/deeplearningdata/archive_1/BraTS2021_Training_Data\")]\n",
    "imgs = []\n",
    "\n",
    "\n",
    "for i in folder_path:\n",
    "    imgs.append(glob(i+\"/*\"))\n",
    "print(\"Done\")\n",
    "folder_path = folder_path[1:]\n",
    "imgs = imgs[1:]\n",
    "print(imgs[0])\n",
    "\n",
    "\n",
    "\n",
    "#input directory and output directory\n",
    "outputpath = []\n",
    "inputpath = []\n",
    "for j in imgs:\n",
    "    for i in j:\n",
    "        if \"seg.nii.gz\" in i.split(\"_\"):\n",
    "            outputpath.append(i)\n",
    "for j in imgs:\n",
    "    temppath = []\n",
    "    for i in j:\n",
    "        if \"seg.nii.gz\" not in i.split(\"_\"):\n",
    "            temppath.append(i)\n",
    "    inputpath.append(temppath)\n",
    "    \n",
    "x_train_path = inputpath[:1126].copy()\n",
    "y_train_path = outputpath[:1126].copy()\n",
    "x_eval_path = inputpath[1126:1189].copy()\n",
    "y_eval_path = outputpath[1126:1189].copy()\n",
    "x_test_path = inputpath[1189:].copy()\n",
    "y_test_path = outputpath[1189:].copy()  \n",
    "\n",
    "parition = {\"train\": x_train_path, \"validation\": x_eval_path}\n",
    "label = {\"train\": y_train_path, \"validation\": y_eval_path}\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------OLD-------------------------------------------------------------------------------\n",
    "# def prepoutput():\n",
    "#     index = 0\n",
    "#     outputimg = np.array((nib.load(outputpath[0]).get_fdata().astype(np.float32)[:, :, 75]))\n",
    "#     outputimg = np.expand_dims(outputimg, axis=2)\n",
    "#     outputimg = tf.image.resize(outputimg, (256, 256))\n",
    "#     for i in range(1,len(outputpath)):\n",
    "#         index+=1\n",
    "#         print(index)\n",
    "#         x = nib.load(outputpath[i]).get_fdata().astype(np.float32)[:, :, 75]\n",
    "#         x = np.expand_dims(x, axis=2)\n",
    "#         x = tf.image.resize(x, (256, 256))\n",
    "#         outputimg = np.dstack((outputimg,x),)\n",
    "#     return outputimg\n",
    "# outputimg = prepoutput()\n",
    "# outputimg = outputimg.transpose((2, 0, 1))\n",
    "# np.save('output_dataset.npy', outputimg)\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "# inputimg = nib.load(inputpath[0][0]).get_fdata().astype(np.float32)[:, :, 75]\n",
    "# for i in range(1,len(inputpath[0])):\n",
    "#     inputimg = np.dstack((inputimg,nib.load(inputpath[0][i]).get_fdata().astype(np.float32)[:, :, 75]))\n",
    "# inputimg = np.expand_dims(inputimg, axis=0)\n",
    "\n",
    "# for i in range(1,len(inputpath)):\n",
    "#     print(i)\n",
    "    \n",
    "#     temp = nib.load(inputpath[i][0]).get_fdata().astype(np.float32)[:, :, 75]\n",
    "#     print(\"\\t1\")\n",
    "#     print(f\"\\t{inputpath[i][0]}\")\n",
    "#     for channels in range(1,len(inputpath[i])):\n",
    "#         print(f\"\\t{channels+1}\")\n",
    "#         print(f\"\\t{inputpath[i][channels]}\")\n",
    "#         temp = np.dstack((temp,nib.load(inputpath[i][channels]).get_fdata().astype(np.float32)[:, :, 75]))\n",
    "        \n",
    "#     temp = np.expand_dims(temp, axis=0)\n",
    "#     inputimg = np.concatenate((inputimg,temp), axis=0)\n",
    "#     print(inputimg.shape)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(240,240,140), n_channels=4,\n",
    "                 n_classes=10, shuffle=True, real_batchsize_custom=2, frames_chunk=18,\n",
    "                 y_VolSlice_start=0, y_VolSlice_end=None):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.real_batchsize_custom = real_batchsize_custom\n",
    "        self.frames_chunk = frames_chunk\n",
    "        self.y_VolSlice_start = y_VolSlice_start\n",
    "        self.y_VolSlice_end = y_VolSlice_end if y_VolSlice_end is not None else dim[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        sliced_dim = (self.dim[0], self.dim[1], self.y_VolSlice_end - self.y_VolSlice_start)\n",
    "        X = np.empty((self.real_batchsize_custom, *sliced_dim, 4))\n",
    "        y = np.empty((self.real_batchsize_custom,*sliced_dim, 4), dtype=float)\n",
    "\n",
    "        initial_point = 0\n",
    "        final_point = self.frames_chunk\n",
    "        for x in range(self.real_batchsize_custom):    \n",
    "            indexes_orig = indexes[initial_point:final_point]\n",
    "            list_IDs_temp = [self.list_IDs[k] for k in indexes_orig]\n",
    "            X_in, y_in = self.__data_generation(list_IDs_temp)\n",
    "            X[x] = X_in\n",
    "            y[x] = y_in\n",
    "            initial_point = final_point\n",
    "            final_point = final_point + self.frames_chunk   \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        sliced_dim = (self.dim[0], self.dim[1], self.y_VolSlice_end - self.y_VolSlice_start)\n",
    "        X = np.empty((1, *sliced_dim, 4))\n",
    "        y = np.zeros((1,*sliced_dim,4))\n",
    "        \n",
    "        random.seed(time.time())\n",
    "\n",
    "        for i, file_paths in enumerate(list_IDs_temp):\n",
    "            image = self.load_nifti(file_paths)\n",
    "                \n",
    "\n",
    "            X[0] = image\n",
    "            \n",
    "            \n",
    "            # Set the label\n",
    "            seg = nib.load(self.labels[i]).get_fdata().astype(np.float16)[:,:,self.y_VolSlice_start:self.y_VolSlice_end]\n",
    "            seg[seg==4] = 3\n",
    "\n",
    "            re= np.expand_dims(seg, axis=0)\n",
    "\n",
    "            re = tf.image.resize(re,(156,156))\n",
    "            re = re[0]\n",
    "            seg = tf.cast(re, tf.int32)\n",
    "            print(seg.shape)\n",
    "            mask = tf.one_hot(seg, 4);\n",
    "            \n",
    "            \n",
    "            y[0] = mask\n",
    "            \n",
    "        X = X.astype(np.float16)\n",
    "        y = y.astype(np.float16)\n",
    "        \n",
    "        return X[0], y[0]\n",
    "\n",
    "    def load_nifti(self, file_paths):\n",
    "        i = 0\n",
    "        temp = nib.load(inputpath[i][0]).get_fdata().astype(np.float16)[:,:,self.y_VolSlice_start:self.y_VolSlice_end]\n",
    "        re= np.expand_dims(temp, axis=0)\n",
    "        re = tf.image.resize(re,(156,156))\n",
    "        temp = re[0]\n",
    "        \n",
    "        temp = np.expand_dims(temp, axis=3)\n",
    "        for channels in range(1,len(file_paths)):\n",
    "            tempo = nib.load(file_paths[channels]).get_fdata().astype(np.float16)[:,:,self.y_VolSlice_start:self.y_VolSlice_end]\n",
    "            re= np.expand_dims(tempo, axis=0)\n",
    "            re = tf.image.resize(re,(156,156))\n",
    "            tempo = re[0]\n",
    "            tempo = np.expand_dims(tempo, axis=3)\n",
    "            temp = np.concatenate((temp,tempo),axis=3)\n",
    "        \n",
    "        return temp\n",
    "\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "#set up var\n",
    "x_VolSlice_start = 10\n",
    "x_VolSlice_end = 150\n",
    "\n",
    "y_VolSlice_start = 10\n",
    "y_VolSlice_end = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad marshal data (unknown type code)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dice_loss(y_true, y_pred) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mcategorical_crossentropy(y_true, y_pred)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load your model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCustomLoss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_loss\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/saving/saving_api.py:176\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    173\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(filepath)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:152\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:170\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 170\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:711\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    714\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    715\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/models/model.py:513\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/models/functional.py:514\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 514\u001b[0m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/models/functional.py:498\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    494\u001b[0m     layer \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[1;32m    495\u001b[0m         layer_data, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m created_layers[layer_name] \u001b[38;5;241m=\u001b[39m layer\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# Gather layer inputs.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:711\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    714\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    715\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/layers/core/lambda_layer.py:192\u001b[0m, in \u001b[0;36mLambda.from_config\u001b[0;34m(cls, config, custom_objects, safe_mode)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_for_lambda_deserialization(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m, safe_mode)\n\u001b[1;32m    191\u001b[0m     inner_config \u001b[38;5;241m=\u001b[39m fn_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 192\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[43mpython_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefaults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclosure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/utils/python_utils.py:83\u001b[0m, in \u001b[0;36mfunc_load\u001b[0;34m(code, defaults, closure, globs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m, binascii\u001b[38;5;241m.\u001b[39mError):\n\u001b[1;32m     82\u001b[0m     raw_code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_unicode_escape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[43mmarshal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m globs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     globs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: bad marshal data (unknown type code)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "keras.config.enable_unsafe_deserialization()\n",
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Define Dice loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "# Combine Dice loss and categorical crossentropy\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return dice_loss(y_true, y_pred) + tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "# Load your model\n",
    "model = tf.keras.models.load_model('best_model.keras', custom_objects={'CustomLoss': combined_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n",
      "start\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def conv_block_3d(input_tensor, num_filters):\n",
    "    x = layers.Conv3D(num_filters, 3, padding='same')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv3D(num_filters, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def unet_3d(input_shape=(156, 156, 140, 4), num_classes=4):\n",
    "    inputs = layers.Input(input_shape, dtype='float16')\n",
    "    \n",
    "    # Encoder (downsampling)\n",
    "    conv1 = conv_block_3d(inputs, 32)\n",
    "    pool1 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "    \n",
    "    conv2 = conv_block_3d(pool1, 64)\n",
    "    pool2 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "    \n",
    "    conv3 = conv_block_3d(pool2, 128)\n",
    "    pool3 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "    \n",
    "    conv4 = conv_block_3d(pool3, 256)\n",
    "    pool4 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv4)\n",
    "    \n",
    "    # Bridge\n",
    "    conv5 = conv_block_3d(pool4, 512)\n",
    "    \n",
    "    # Decoder (upsampling)\n",
    "    up6 = layers.Conv3DTranspose(256, 2, strides=(2, 2, 2), padding='same')(conv5)\n",
    "    up6 = layers.Lambda(lambda x: tf.pad(x, [[0, 0], [0, conv4.shape[1] - x.shape[1]], [0, conv4.shape[2] - x.shape[2]], [0, conv4.shape[3] - x.shape[3]], [0, 0]]))(up6)\n",
    "    concat6 = layers.Concatenate()([up6, conv4])\n",
    "    conv6 = conv_block_3d(concat6, 256)\n",
    "    \n",
    "    up7 = layers.Conv3DTranspose(128, 2, strides=(2, 2, 2), padding='same')(conv6)\n",
    "    up7 = layers.Lambda(lambda x: tf.pad(x, [[0, 0], [0, conv3.shape[1] - x.shape[1]], [0, conv3.shape[2] - x.shape[2]], [0, conv3.shape[3] - x.shape[3]], [0, 0]]))(up7)\n",
    "    concat7 = layers.Concatenate()([up7, conv3])\n",
    "    conv7 = conv_block_3d(concat7, 128)\n",
    "    \n",
    "    up8 = layers.Conv3DTranspose(64, 2, strides=(2, 2, 2), padding='same')(conv7)\n",
    "    up8 = layers.Lambda(lambda x: tf.pad(x, [[0, 0], [0, conv2.shape[1] - x.shape[1]], [0, conv2.shape[2] - x.shape[2]], [0, conv2.shape[3] - x.shape[3]], [0, 0]]))(up8)\n",
    "    concat8 = layers.Concatenate()([up8, conv2])\n",
    "    conv8 = conv_block_3d(concat8, 64)\n",
    "    \n",
    "    up9 = layers.Conv3DTranspose(32, 2, strides=(2, 2, 2), padding='same')(conv8)\n",
    "    up9 = layers.Lambda(lambda x: tf.pad(x, [[0, 0], [0, conv1.shape[1] - x.shape[1]], [0, conv1.shape[2] - x.shape[2]], [0, conv1.shape[3] - x.shape[3]], [0, 0]]))(up9)\n",
    "    concat9 = layers.Concatenate()([up9, conv1])\n",
    "    conv9 = conv_block_3d(concat9, 32)\n",
    "    \n",
    "    outputs = layers.Conv3D(num_classes, 1, activation='softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Define Dice coefficient\n",
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Define Dice loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "# Combine Dice loss and categorical crossentropy\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return dice_loss(y_true, y_pred) + tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "print(\"start\")\n",
    "traingen = DataGenerator(parition[\"train\"],label[\"train\"],5,(156,156,140),1,4,True,y_VolSlice_start=10,y_VolSlice_end=150)\n",
    "validationgen = DataGenerator(parition[\"validation\"],label[\"validation\"],5,(156,156,140),1,4,True,y_VolSlice_start=10,y_VolSlice_end=150)\n",
    "# Create the model\n",
    "with strategy.scope():\n",
    "    model = unet_3d()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=combined_loss,\n",
    "              metrics=[dice_coefficient, 'accuracy',tf.keras.metrics.MeanIoU(4)])\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "print(\"here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/red/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 166 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
